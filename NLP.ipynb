{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e276ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Sneha\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd99e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization breaks the raw text into words, sentences called token\n",
    "# The meaning of the test could be easily interpreted by the machine\n",
    "# word tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8893d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Best', 'quality', ',', 'Great', 'sound', ',', 'Good', 'comfort', 'with', 'superb', 'Headphone', 'Design', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Best quality, Great sound, Good comfort with superb Headphone Design.'\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddefdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think built quality is okay in this price range.', 'It totally made of plastic.', 'But it is flexible for all size of heads.', 'Also you can adjust headphone size as your need.', \"Type C port for recharge battery, but it's not a USB port.\", 'I mean you can not load songs from your computer to memory card using type C port.', \"It's just for charging purpose.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"think built quality is okay in this price range. It totally made of plastic. But it is flexible for all size of heads. Also you can adjust headphone size as your need. Type C port for recharge battery, but it's not a USB port. I mean you can not load songs from your computer to memory card using type C port. It's just for charging purpose.\" \n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dca307d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Best',\n",
       " 'quality,',\n",
       " 'Great',\n",
       " 'sound,',\n",
       " 'Good',\n",
       " 'comfort',\n",
       " 'with',\n",
       " 'superb',\n",
       " 'Headphone',\n",
       " 'Design.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Best quality, Great sound, Good comfort with superb Headphone Design.'\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acc61827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sneha\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a539a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ddbec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"think built quality is okay in this price range. It totally made of plastic. But it is flexible for all size of heads. Also you can adjust headphone size as your need. Type C port for recharge battery, but it's not a USB port. I mean you can not load songs from your computer to memory card using type C port. It's just for charging purpose.\" \n",
    "word_in_text = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd317421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think',\n",
       " 'built',\n",
       " 'quality',\n",
       " 'okay',\n",
       " 'price',\n",
       " 'range',\n",
       " '.',\n",
       " 'totally',\n",
       " 'made',\n",
       " 'plastic',\n",
       " '.',\n",
       " 'flexible',\n",
       " 'size',\n",
       " 'heads',\n",
       " '.',\n",
       " 'Also',\n",
       " 'adjust',\n",
       " 'headphone',\n",
       " 'size',\n",
       " 'need',\n",
       " '.',\n",
       " 'Type',\n",
       " 'C',\n",
       " 'port',\n",
       " 'recharge',\n",
       " 'battery',\n",
       " ',',\n",
       " \"'s\",\n",
       " 'USB',\n",
       " 'port',\n",
       " '.',\n",
       " 'mean',\n",
       " 'load',\n",
       " 'songs',\n",
       " 'computer',\n",
       " 'memory',\n",
       " 'card',\n",
       " 'using',\n",
       " 'type',\n",
       " 'C',\n",
       " 'port',\n",
       " '.',\n",
       " \"'s\",\n",
       " 'charging',\n",
       " 'purpose',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter_list = []\n",
    "# for word in word_in_text:\n",
    "#     if word.casefold() not in stop_words:\n",
    "#         filter_list.append(word)\n",
    "# filter_list\n",
    "\n",
    "filter_list = [word for word in word_in_text if word.casefold() not in stop_words]\n",
    "filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c67f58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "# Stemming reduces the complex words to the root word, eg; 'chocolatey', 'choco' to 'chocolate'\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['wait','waiting','waited','waits']\n",
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0a5173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sneha\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5494334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "waiting\n",
      "waited\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer\n",
    "# Stemming reduces the complex words to the root word, eg; 'chocolatey', 'choco' to 'chocolate'\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['wait','waiting','waited','waits']\n",
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "275d76cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discoveri\n",
      "discov\n",
      "discoveri\n",
      "discov\n"
     ]
    }
   ],
   "source": [
    "words = ['Discovery','discovered','discoveries','Discovering']\n",
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3956c8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovery\n",
      "discovered\n",
      "discovery\n",
      "Discovering\n"
     ]
    }
   ],
   "source": [
    "words = ['Discovery','discovered','discoveries','Discovering']\n",
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "239b01c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Sneha Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "014537af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part-of-speech tagging [pos tagging] --mark each word as noun, adjective, etc\n",
    " \n",
    "text = 'the quick brown fox jumps over the little lazy dog'\n",
    "words = word_tokenize(text)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8765993c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 624.3/624.3 kB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64508f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "# TextBlob.correct() method\n",
    "# will correct the spelling mistakes, but the % is not that high\n",
    "from textblob import TextBlob\n",
    "text = TextBlob(\"I havv gooood speeling!\")\n",
    "print(text.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b42c33dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XYZ is a good company and always value their employees.\n"
     ]
    }
   ],
   "source": [
    "test = TextBlob('XYZ is a good compny and alays valule ttheir employees.')\n",
    "test = test.correct()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "982118ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U chin bid had mast mast\n"
     ]
    }
   ],
   "source": [
    "test = TextBlob('Tu chij bdi hai mast mast')\n",
    "test = test.correct()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1978995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting googletrans\n",
      "  Downloading googletrans-4.0.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting httpx>=0.27.2 (from httpx[http2]>=0.27.2->googletrans)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.6.2)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.14.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.27.2->googletrans)\n",
      "  Downloading h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans)\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans)\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.0)\n",
      "Downloading googletrans-4.0.2-py3-none-any.whl (18 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: hyperframe, hpack, httpx, h2, googletrans\n",
      "Successfully installed googletrans-4.0.2 h2-4.2.0 hpack-4.1.0 httpx-0.28.1 hyperframe-6.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\Sneha Gupta\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script translate.exe is installed in 'C:\\Users\\Sneha Gupta\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74158578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abk': 'abkhaz',\n",
       " 'ace': 'acehnese',\n",
       " 'ach': 'acholi',\n",
       " 'aar': 'afar',\n",
       " 'af': 'afrikaans',\n",
       " 'sq': 'albanian',\n",
       " 'alz': 'alur',\n",
       " 'am': 'amharic',\n",
       " 'ar': 'arabic',\n",
       " 'hy': 'armenian',\n",
       " 'as': 'assamese',\n",
       " 'ava': 'avar',\n",
       " 'awa': 'awadhi',\n",
       " 'ay': 'aymara',\n",
       " 'az': 'azerbaijani',\n",
       " 'ban': 'balinese',\n",
       " 'bal': 'baluchi',\n",
       " 'bm': 'bambara',\n",
       " 'bci': 'baoulé',\n",
       " 'bak': 'bashkir',\n",
       " 'eu': 'basque',\n",
       " 'btx': 'batak karo',\n",
       " 'bts': 'batak simalungun',\n",
       " 'bbc': 'batak toba',\n",
       " 'be': 'belarusian',\n",
       " 'bem': 'bemba',\n",
       " 'bn': 'bengali',\n",
       " 'bew': 'betawi',\n",
       " 'bho': 'bhojpuri',\n",
       " 'bik': 'bikol',\n",
       " 'bs': 'bosnian',\n",
       " 'bre': 'breton',\n",
       " 'bg': 'bulgarian',\n",
       " 'bua': 'buryat',\n",
       " 'yue': 'cantonese',\n",
       " 'ca': 'catalan',\n",
       " 'ceb': 'cebuano',\n",
       " 'cha': 'chamorro',\n",
       " 'che': 'chechen',\n",
       " 'zh': 'chinese',\n",
       " 'zh-cn': 'chinese (simplified)',\n",
       " 'zh-tw': 'chinese (traditional)',\n",
       " 'chk': 'chuukese',\n",
       " 'chv': 'chuvash',\n",
       " 'co': 'corsican',\n",
       " 'crh': 'crimean tatar',\n",
       " 'hr': 'croatian',\n",
       " 'cs': 'czech',\n",
       " 'da': 'danish',\n",
       " 'fa-af': 'dari',\n",
       " 'dv': 'dhivehi',\n",
       " 'din': 'dinka',\n",
       " 'doi': 'dogri',\n",
       " 'dom': 'dombe',\n",
       " 'nl': 'dutch',\n",
       " 'dyu': 'dyula',\n",
       " 'dzo': 'dzongkha',\n",
       " 'en': 'english',\n",
       " 'eo': 'esperanto',\n",
       " 'et': 'estonian',\n",
       " 'fao': 'faroese',\n",
       " 'fij': 'fijian',\n",
       " 'fil': 'filipino (tagalog)',\n",
       " 'fi': 'finnish',\n",
       " 'fon': 'fon',\n",
       " 'fr': 'french',\n",
       " 'fy': 'frisian',\n",
       " 'fur': 'friulian',\n",
       " 'ful': 'fulani',\n",
       " 'gaa': 'ga',\n",
       " 'gl': 'galician',\n",
       " 'ka': 'georgian',\n",
       " 'de': 'german',\n",
       " 'el': 'greek',\n",
       " 'gn': 'guarani',\n",
       " 'gu': 'gujarati',\n",
       " 'ht': 'haitian creole',\n",
       " 'cnh': 'hakha chin',\n",
       " 'ha': 'hausa',\n",
       " 'haw': 'hawaiian',\n",
       " 'he': 'hebrew',\n",
       " 'iw': 'hebrew',\n",
       " 'hil': 'hiligaynon',\n",
       " 'hi': 'hindi',\n",
       " 'hmn': 'hmong',\n",
       " 'hu': 'hungarian',\n",
       " 'hrx': 'hunsrik',\n",
       " 'iba': 'iban',\n",
       " 'is': 'icelandic',\n",
       " 'ig': 'igbo',\n",
       " 'ilo': 'ilocano',\n",
       " 'id': 'indonesian',\n",
       " 'ga': 'irish',\n",
       " 'it': 'italian',\n",
       " 'jam': 'jamaican patois',\n",
       " 'ja': 'japanese',\n",
       " 'jv': 'javanese',\n",
       " 'jw': 'javanese',\n",
       " 'kac': 'jingpo',\n",
       " 'kal': 'kalaallisut',\n",
       " 'kn': 'kannada',\n",
       " 'kau': 'kanuri',\n",
       " 'pam': 'kapampangan',\n",
       " 'kk': 'kazakh',\n",
       " 'kha': 'khasi',\n",
       " 'km': 'khmer',\n",
       " 'cgg': 'kiga',\n",
       " 'kik': 'kikongo',\n",
       " 'rw': 'kinyarwanda',\n",
       " 'ktu': 'kituba',\n",
       " 'trp': 'kokborok',\n",
       " 'kom': 'komi',\n",
       " 'gom': 'konkani',\n",
       " 'ko': 'korean',\n",
       " 'kri': 'krio',\n",
       " 'ku': 'kurdish',\n",
       " 'ckb': 'kurdish (sorani)',\n",
       " 'ky': 'kyrgyz',\n",
       " 'lo': 'lao',\n",
       " 'ltg': 'latgalian',\n",
       " 'la': 'latin',\n",
       " 'lv': 'latvian',\n",
       " 'lij': 'ligurian',\n",
       " 'lim': 'limburgish',\n",
       " 'ln': 'lingala',\n",
       " 'lt': 'lithuanian',\n",
       " 'lmo': 'lombard',\n",
       " 'lg': 'luganda',\n",
       " 'luo': 'luo',\n",
       " 'lb': 'luxembourgish',\n",
       " 'mk': 'macedonian',\n",
       " 'mad': 'madurese',\n",
       " 'mai': 'maithili',\n",
       " 'mak': 'makassar',\n",
       " 'mg': 'malagasy',\n",
       " 'ms': 'malay',\n",
       " 'ms-arab': 'malay (jawi)',\n",
       " 'ml': 'malayalam',\n",
       " 'mt': 'maltese',\n",
       " 'mam': 'mam',\n",
       " 'glv': 'manx',\n",
       " 'mi': 'maori',\n",
       " 'mr': 'marathi',\n",
       " 'mah': 'marshallese',\n",
       " 'mwr': 'marwadi',\n",
       " 'mfe': 'mauritian creole',\n",
       " 'mhr': 'meadow mari',\n",
       " 'mni-mtei': 'meiteilon (manipuri)',\n",
       " 'min': 'minang',\n",
       " 'lus': 'mizo',\n",
       " 'mn': 'mongolian',\n",
       " 'my': 'myanmar (burmese)',\n",
       " 'nhe': 'nahuatl (eastern huasteca)',\n",
       " 'ndc-zw': 'ndau',\n",
       " 'nde': 'ndebele (south)',\n",
       " 'new': 'nepalbhasa (newari)',\n",
       " 'ne': 'nepali',\n",
       " 'no': 'norwegian',\n",
       " 'nus': 'nuer',\n",
       " 'ny': 'nyanja (chichewa)',\n",
       " 'oci': 'occitan',\n",
       " 'or': 'odia (oriya)',\n",
       " 'om': 'oromo',\n",
       " 'oss': 'ossetian',\n",
       " 'pag': 'pangasinan',\n",
       " 'pap': 'papiamento',\n",
       " 'ps': 'pashto',\n",
       " 'fa': 'persian',\n",
       " 'pl': 'polish',\n",
       " 'por': 'portuguese (portugal)',\n",
       " 'pt': 'portuguese (portugal, brazil)',\n",
       " 'pa': 'punjabi',\n",
       " 'pa-arab': 'punjabi (shahmukhi)',\n",
       " 'kek': \"q'eqchi'\",\n",
       " 'qu': 'quechua',\n",
       " 'rom': 'romani',\n",
       " 'ro': 'romanian',\n",
       " 'run': 'rundi',\n",
       " 'ru': 'russian',\n",
       " 'sme': 'sami (north)',\n",
       " 'sm': 'samoan',\n",
       " 'sag': 'sango',\n",
       " 'sa': 'sanskrit',\n",
       " 'sat': 'santali',\n",
       " 'gd': 'scots gaelic',\n",
       " 'nso': 'sepedi',\n",
       " 'sr': 'serbian',\n",
       " 'st': 'sesotho',\n",
       " 'crs': 'seychellois creole',\n",
       " 'shn': 'shan',\n",
       " 'sn': 'shona',\n",
       " 'scn': 'sicilian',\n",
       " 'szl': 'silesian',\n",
       " 'sd': 'sindhi',\n",
       " 'si': 'sinhala (sinhalese)',\n",
       " 'sk': 'slovak',\n",
       " 'sl': 'slovenian',\n",
       " 'so': 'somali',\n",
       " 'es': 'spanish',\n",
       " 'su': 'sundanese',\n",
       " 'sus': 'susu',\n",
       " 'sw': 'swahili',\n",
       " 'ssw': 'swati',\n",
       " 'sv': 'swedish',\n",
       " 'tl': 'tagalog (filipino)',\n",
       " 'tah': 'tahitian',\n",
       " 'tg': 'tajik',\n",
       " 'ber-atn': 'tamazight',\n",
       " 'ber': 'tamazight (tifinagh)',\n",
       " 'ta': 'tamil',\n",
       " 'tt': 'tatar',\n",
       " 'te': 'telugu',\n",
       " 'tet': 'tetum',\n",
       " 'th': 'thai',\n",
       " 'bod': 'tibetan',\n",
       " 'ti': 'tigrinya',\n",
       " 'tiv': 'tiv',\n",
       " 'tpi': 'tok pisin',\n",
       " 'ton': 'tongan',\n",
       " 'ts': 'tsonga',\n",
       " 'tsn': 'tswana',\n",
       " 'tcy': 'tulu',\n",
       " 'tum': 'tumbuka',\n",
       " 'tr': 'turkish',\n",
       " 'tk': 'turkmen',\n",
       " 'tuk': 'tuvan',\n",
       " 'ak': 'twi (akan)',\n",
       " 'udm': 'udmurt',\n",
       " 'uk': 'ukrainian',\n",
       " 'ur': 'urdu',\n",
       " 'ug': 'uyghur',\n",
       " 'uz': 'uzbek',\n",
       " 'ven': 'venda',\n",
       " 'vec': 'venetian',\n",
       " 'vi': 'vietnamese',\n",
       " 'war': 'waray',\n",
       " 'cy': 'welsh',\n",
       " 'wol': 'wolof',\n",
       " 'xh': 'xhosa',\n",
       " 'sah': 'yakut',\n",
       " 'yi': 'yiddish',\n",
       " 'yo': 'yoruba',\n",
       " 'yua': 'yucatec maya',\n",
       " 'zap': 'zapotec',\n",
       " 'zu': 'zulu'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googletrans\n",
    "googletrans.LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "962ff4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19d57fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you?\n"
     ]
    }
   ],
   "source": [
    "from_lang = 'mr'\n",
    "to_lang = 'en'\n",
    "text = 'तुम्ही कसे आहात?'\n",
    "res = await translator.translate(text, src = from_lang, dest = to_lang)\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "408d37af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     -------------------- ----------------- 524.3/981.5 kB 3.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 4.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=0f036598d78ac2158069ae31214dbddbd18c84a009e9ea4c7c57a9419fded991\n",
      "  Stored in directory: c:\\users\\sneha gupta\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f1706ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "[hi:0.9999989673722502]\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "print(detect('आप कैसे हैं?'))\n",
    "print(detect_langs('आप कैसे हैं?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a195c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
